// Web scraper showing async generators and concurrent fetching

struct Page {
    let url: String
    let title: String
    let links: [String]
    let nextPage: Option[String]
}

async func fetchPage(url: String) -> Page throws HttpError {
    let response = await http.get(url)
    guard response.status == 200 else {
        throw HttpError.status(response.status)
    }

    let html = await response.text()
    let title = html.extractBetween("<title>", "</title>") ?? "Untitled"
    let links = html.extractAll("href=\"", "\"")
    let nextPage = html.extractBetween("rel=\"next\" href=\"", "\"")

    Page(url: url, title: title, links: links, nextPage: nextPage)
}

async generator func crawlPages(startUrl: String) -> Page throws HttpError {
    var current: Option[String] = .Some(startUrl)
    var visited: Set[String] = Set()

    while let Some(url) = current {
        if visited.contains(url) {
            current = .None
            continue
        }

        visited.insert(url)
        let page = try await fetchPage(url)
        yield page
        current = page.nextPage
    }
}

async func scrapeAll(urls: [String]) -> [Page] {
    // Fetch all URLs concurrently
    let tasks = urls.map { url in
        spawn {
            try await fetchPage(url)
        }
    }

    // Collect results, filtering out failures
    var pages: [Page] = []
    for task in tasks {
        match await task {
            Ok(page) => pages.push(page)
            Err(_) => {}
        }
    }
    pages
}

func main() {
    given Executor = WorkStealingExecutor(threads: 8)

    Executor.block {
        // Crawl paginated content
        print("Crawling blog posts...")
        async for page in crawlPages("https://example.com/blog") {
            print("  \(page.title)")
        }

        // Scrape multiple URLs concurrently
        print("\nFetching product pages...")
        let urls = for i in 1 to 10 yield "https://example.com/product/\(i)"
        let products = await scrapeAll(urls.collect())

        for page in products {
            print("  \(page.title) - \(page.links.length) links")
        }

        print("\nDone! Scraped \(products.length) products")
    }
}
